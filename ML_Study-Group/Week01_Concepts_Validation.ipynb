{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 01 Concepts: Environment Setup\n",
    "\n",
    "## BEENOS Inc. Machine Learning Study Group\n",
    "\n",
    "In this first week, we'll continue our previous discussion on validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Last week, we worked through [a short machine learning project in Python](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/).\n",
    "\n",
    "In Step 5, we created a validation set and used cross-fold validation to evaluate our model.\n",
    "\n",
    "This week, we'll learn more about validation, what it is, and why it's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why validate?\n",
    "\n",
    "Validation tells us how well our model performs on data.\n",
    "\n",
    "Let's say we're going to build a classifier. Below is a support vector machine trained on our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/f0499405-1197-4b43-b7c5-40548eeb9f34/Image/3b3ca365c0a481a5fa29437947107aaf/machine_learning_classification_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 misclassifications (3 blue dots on the red side, and 3 red dots on the blue side).\n",
    "\n",
    "There are 33 points in total, and the classifier mistook 6 of them. That means it got 27 right. So you might say that it works 27/33, or 81.8% of the time.\n",
    "\n",
    "Unfortunately, that number is probably too high.\n",
    "\n",
    "That's because **all data contains some amount of randomness.**\n",
    "\n",
    "There is clearly a real pattern in the dataset above: the blue dots tend to cluster on the right, while the red dots tend to be on the left. However, there are also random patterns that exist in data as well.\n",
    "\n",
    "When we train a model on a dataset, we're training it to predict real effects as well as random effects.\n",
    "\n",
    "If we evaluate the model on the same data we trained it on, it's going to do really well, because it's fit to the random effects of that data.\n",
    "\n",
    "However, when we try to use the model on *new, unseen* data, it might perform much more poorly. This is because the \"handicap\" of the training set is gone: the model can only work with the real effects present in the new data. We might discover that, in actuality, the model does very poorly.\n",
    "\n",
    "If we'd known this beforehand, we could have improved our model, or tried a different algorithm, to get a model that works well on lots of different data sets, not just our training set.\n",
    "\n",
    "Validation may not seem like a big deal; so what if it *really* performs with an accuracy of 75% instead of 81%?\n",
    "\n",
    "Well, the question becomes more important when you start to classify life-or-death situations. Would you trust a model that tells you whether your cells are cancerous, if it was only right 81% of the time?\n",
    "\n",
    "What if you later found out that that model was really only 75% of the time, and the analysts just hadn't evaluated it correctly?\n",
    "\n",
    "I thought so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real vs. Randomness\n",
    "\n",
    "It can be difficult to grasp what \"random effects\" are, so we'll take a look at an example from the Georgia Institute of Technology's online course, Intro to Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 0:\n",
    "\n",
    "Think of 10 people whose birthdays you know. For each person, replace one of the `0`s in the list below with the day of the month that person was born."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybirthdays = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    0\n",
    "]\n",
    "\n",
    "mybirthdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "Look at your birthday list. Now, if you were to meet a new person (say, me!), and had to guess what day of the month they were born on, what day would you pick? Why?\n",
    "\n",
    "Fill in your prediction below. Change the `0` to the day of the month you guess a stranger might be born on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myguess = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "Let's see how wrong you were! We'll use the same data to evaluate our model performance.\n",
    "\n",
    "(Remember, this is NOT what you should do!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(bday_list, myguess):\n",
    "    error = []\n",
    "\n",
    "    for bday in bday_list:\n",
    "        error.append(abs(bday - myguess))\n",
    "        \n",
    "    return sum(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = get_error(mybirthdays, myguess)\n",
    "\n",
    "print(\"My model should perform this well at guessing birthdays: \", performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did your model do? The closer to 0, the better!\n",
    "\n",
    "So, to summarize:\n",
    "\n",
    "1. You chose a dataset\n",
    "2. You trained a model (made a guess) on that dataset\n",
    "3. You evaluated the model on that same dataset (**a big no-no!**)\n",
    "4. You determined your model's error\n",
    "\n",
    "Now you know how well you believe your model to do, let's see how well it *actually* does on different data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3:\n",
    "\n",
    "Below I've prepared a few datasets for you. Run the code cells to evaluate your model (your guess) on these new, unseen datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    \"my_birthdays\" : mybirthdays,\n",
    "    \"jaya_birthdays\" : [21, 20, 22, 16, 6, 9, 10, 21, 10, 24],\n",
    "    \"gatech_birthdays\" : [3, 21, 24, 24, 25, 26, 27, 30, 30, 31],\n",
    "    \"perfect_birthdays\" : [myguess]*10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bday_list in new_data:\n",
    "    error = get_error(new_data[bday_list], myguess)\n",
    "    print(\"My model performs this well at guessing \" + bday_list + \":\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your model (your guess) performing better or worse? (You can see that, if you were to randomly get a \"perfect\" dataset, where all your respondents were born on the day you guessed, your error rate would be 0! But is it really guessing birthdays \"perfectly\"?)\n",
    "\n",
    "Chances are it's not doing too well on these new data sets! That's because, in reality, people are born uniformly throughout the month. Any \"pattern\" you may see otherwise is due to one thing: **a random effect**.\n",
    "\n",
    "Here are some random birthday patterns. You may have used one of these to choose `myguess`.\n",
    "\n",
    "When taking a survey, birthdays may *appear* to be:\n",
    "\n",
    "* clustered around the beginning of the month\n",
    "* clustered within the middle of the month\n",
    "* clustered towards the end of the month\n",
    "* mostly even-numbered days\n",
    "* mostly odd-numbered days\n",
    "* days that are multiples of 2 or 3\n",
    "* close to when your friends were born\n",
    "* close to when your loved ones were born (<-- my dataset was like this!)\n",
    "\n",
    "If you evaluated your model on only one dataset, and it was fit to one of the random patterns above, then when it comes up against a new dataset, it's not going to perform very well!\n",
    "\n",
    "One way we can fix this is to **validate our model** by pitting it against *multiple* datasets, with different random patterns. For instance, we might fit our birthday model to:\n",
    "\n",
    "* your random pattern\n",
    "* a dataset with birthdays that are only even-numbered days\n",
    "* a dataset with birthdays all in the 30s\n",
    "* a dataset with birthdays that are multiples of 2\n",
    "\n",
    "Each time, we would re-train our model (update our guess) so that it does a better job on each new dataset it sees. This makes our model more robust to the random patterns in each dataset, and helps it make better predictions.\n",
    "\n",
    "This is called **validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation\n",
    "\n",
    "So, we know we can't measure a model's quality on the data it was trained on. This means that, at the very least, **we need to split our data into at least two sets: one for training, and one for validation.**\n",
    "\n",
    "*Note*: If you're only making a 2-way split, then the validation set could be called a validation set *or* a test set, because we're \"testing\" how well the model performs. This distinction can be confusing, but many people will refer to it as both, so keep it in mind!\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n",
    "\n",
    "When we use a second data set to validate our model, we figure out how well it *really* does, without accounting for the random pattern it learned through the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:\n",
    "\n",
    "Let's train your birthday model again. But this time, we won't measure `performance` on the dataset you used to pick the guess. We'll measure it on a new, random dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the trained model\n",
    "\n",
    "print(\"I trained my model on my birthdays: \", mybirthdays)\n",
    "print(\"The model I chose was: \", myguess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on a different data set\n",
    "\n",
    "validation_birthdays = [31, 15, 23, 26, 23, 4, 5, 24, 19, 18]\n",
    "\n",
    "performance = get_error(validation_birthdays, myguess)\n",
    "\n",
    "print(\"My model ACTUALLY performs this well at guessing birthdays: \", performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Is that a better indicator of how your model really performed?\n",
    "\n",
    "When I tried this exercise, the performance I got on the training set was 57, and on the validation set, 72! I definitely think that my error rate was much too optimistic. My guess was worse than I had thought!\n",
    "\n",
    "**If I hadn't used a validation set to measure performance, I would not have known this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test\n",
    "\n",
    "Now, in many cases, a simple train/test (or train/validation) split will suffice: train your model on some data, and evaluate performance on different data.\n",
    "\n",
    "However, most machine learning projects will require developing many different models, and choosing the best one to proceed with making predictions; for instance, training SVM, KNN and Logistic Regression classifiers on the iris flowers dataset. Of the three, which one is the best model? Which one should you use?\n",
    "\n",
    "Well, we know that we should really evaluate performance on different data. So we might make several models and evaluate them all on a validation set.\n",
    "\n",
    "Let's try this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5:\n",
    "\n",
    "Use your birthday list to come up with two new guesses. This way, you'll have three different birthday \"models\" to choose from. Replace the `0`s in the code block below with your guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the trained model\n",
    "\n",
    "print(\"I trained my model on my birthdays: \", mybirthdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthday_guess1 = myguess\n",
    "birthday_guess2 = 0\n",
    "birthday_guess3 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use our validation birthday set to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the models on a different data set\n",
    "\n",
    "validation_birthdays = [31, 15, 23, 26, 23, 4, 5, 24, 19, 18]\n",
    "\n",
    "model1_performance = get_error(validation_birthdays, birthday_guess1)\n",
    "model2_performance = get_error(validation_birthdays, birthday_guess2)\n",
    "model3_performance = get_error(validation_birthdays, birthday_guess3)\n",
    "\n",
    "print(\"birthday_guess1 should perform this well at guessing birthdays: \", model1_performance)\n",
    "print(\"birthday_guess2 should perform this well at guessing birthdays: \", model2_performance)\n",
    "print(\"birthday_guess3 should perform this well at guessing birthdays: \", model3_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model has the lowest error rate? This is the model you predict will make good birthday predictions. Store it in the cell below. Replace the `mynewguess` after the `=` with the variable of the model that performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynewguess = mynewguess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6:\n",
    "\n",
    "Let's see how your chosen model does on a completely new, random set of birthdays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the validated model on a different data set\n",
    "\n",
    "test_birthdays = [22, 24, 31, 3, 12, 17, 9, 12, 27, 30]\n",
    "\n",
    "performance = get_error(test_birthdays, mynewguess)\n",
    "\n",
    "print(\"My model ACTUALLY performs this well at guessing birthdays: \", performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your model better or worse?\n",
    "\n",
    "My model performance is actually still worse! On the validation set, the best model's error was 66; but on the test set, that number skyrocketed to 81!\n",
    "\n",
    "This is because **even the validation set has some random effects in it.** (In this case, the \"effect\" is whatever numbers were randomly popping up in my head as I typed them.)\n",
    "\n",
    "The **test set also has some randomness**, whatever digital randomness was going on in the online random calendar date generator I used to populate the list.\n",
    "\n",
    "**Just because the error LOOKS good, doesn't mean the model will actually perform well!**\n",
    "\n",
    "Suppose you came up with 10 birthday guesses, and they all truly did have the same performance on the validation set:\n",
    "\n",
    "![](./imgs/Week01_Concepts_random-bdays.png)\n",
    "\n",
    "We can only see the total effect; we can't see the real effects. So, picking a model based on how we see it perform on the validation set is still too optimistic. In the graphic above, the best model also just so happens to have the most randomness, making it \"look\" the best. In reality, all the models perform equally well, and the \"best\" model's quality is actually much lower than what we see. We picked it just on luck.\n",
    "\n",
    "This is why we use a test set as well: to confirm whether or not the model we chose really *is* as good as it looks.\n",
    "\n",
    "Georgia Tech has provided the following flowchart to help determine whether you need a train/test (also called train/validation) split, or a train/validation/test split:\n",
    "\n",
    "![](./imgs/Week01_Concepts_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "In reality, we won't be using three different data sets to evaluate our model. Rather, we would have one large dataset and need to split it into 2 or 3.\n",
    "\n",
    "So our birthday data set would actually look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthdays = [21, 20, 22, 16, 6, 9, 10, 21, 10, 24, \n",
    "             3, 21, 24, 24, 25, 26, 27, 30, 30, 31, \n",
    "             17, 17, 17, 17, 17, 17, 17, 17, 17, 17, \n",
    "             31, 15, 23, 26, 23, 4, 5, 24, 19, 18, \n",
    "             22, 24, 31, 3, 12, 17, 9, 12, 27, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we would split the data set into 80/20 train/test or 70/15/15 train/validation/test sets.\n",
    "\n",
    "But when we do this, the amount of data we have to train our model on gets much smaller.\n",
    "\n",
    "Why might this be a bad idea?\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Throughout this notebook, we've been talking about birthday guesses that perform really well on test data sets, and really poorly on others. We can say that our models or guesses **overfit** the data we train them on.\n",
    "\n",
    "Overfitting the data becomes a problem the fewer data points you have to observe.\n",
    "\n",
    "#### Exercise 7:\n",
    "\n",
    "What birthday guess would you come up with for the first data set? The second one? Which data set do you think your model/guess would perform better on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set1 = [3, 4, 2, 3, 1, 5]\n",
    "\n",
    "training_set2 = [21, 20, 22, 16, 6, 9, 10, 21, 10, 24, \n",
    "             3, 21, 24, 24, 25, 26, 27, 30, 30, 31, \n",
    "             17, 17, 17, 17, 17, 17, 17, 17, 17, 17, \n",
    "             31, 15, 23, 26, 23, 4, 5, 24, 19, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your guess/model would probably do better on the first set, simply because there are fewer points to make mistakes!\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*tBErXYVvTw2jSUYK7thU2A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building models, we want to make sure that our guesses will be \"just right.\" We don't want them to be way off the mark, but we don't necessarily want them to be \"perfect\" either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross-Validation\n",
    "\n",
    "This is a technique that we can use to still validate our models, even when we have a very small number of data points.\n",
    "\n",
    "The process starts by splitting a large portion of the data set into a piece that will be used for BOTH training and validation. Then, you further split that piece into `k` smaller pieces.\n",
    "\n",
    "![](./imgs/Week01_Concepts_crossval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll take `k-1` of those parts and train the model on them, then you'll validate it on the remaining part.\n",
    "\n",
    "**Then, you repeat this process until all parts of the data set have been used to train and validate.**\n",
    "\n",
    "![](./imgs/Week01_Concepts_kfold.png)\n",
    "\n",
    "When you're ready to evaluate the model, you choose the best guess and then **re-train the model on ALL data points together.** This way, we can train a model knowing that the one we chose is likely to have the best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
